{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijgez6ADXwlH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. What is a random variable in probability theory?**  \n",
        "A *random variable* is a function that assigns a numerical value to each possible outcome of a random experiment within a sample space. It transforms uncertain outcomes into real numbers, which allows us to analyze and quantify randomness mathematically. For example, in rolling a die, the random variable could be the number shown on the face, taking values 1 through 6. Random variables can be classified into *discrete* or *continuous* types based on the nature of their possible values.\n",
        "\n",
        "**2. What are the types of random variables?**  \n",
        "- **Discrete Random Variables:** These take countable, distinct values, such as integers or specific categories. For example, the number of cars passing through an intersection in an hour or the number of students in a classroom. Their probability distributions assign probabilities to each possible value.  \n",
        "- **Continuous Random Variables:** These can take any value within a continuous range or interval, such as height, weight, or temperature. Their probability distributions are described by density functions, and the probability of the variable taking an exact value is zero; instead, probabilities are assigned over intervals.\n",
        "\n",
        "**3. What is the difference between discrete and continuous distributions?**  \n",
        "Discrete distributions describe the probability of specific, separate values. The probability mass function (PMF) assigns a probability to each possible outcome, and the sum of all probabilities equals 1. Examples include the binomial and Poisson distributions.  \n",
        "Continuous distributions describe variables that can take any value within a range. Their probability density function (PDF) provides the density at each point, and probabilities are calculated over intervals by integrating the PDF. The probability at any single point is zero, but the probability over an interval is the area under the PDF curve.\n",
        "\n",
        "**4. What are probability distribution functions (PDF)?**  \n",
        "In the context of continuous random variables, the *probability density function* (PDF) describes how the probabilities are distributed over the possible values. The PDF assigns a density to each point, and the probability that the variable falls within a specific interval is given by the integral of the PDF over that interval. The key property is that the total area under the PDF over the entire space equals 1, representing total certainty that the variable falls somewhere within its range.\n",
        "\n",
        "**5. How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)?**  \n",
        "The *cumulative distribution function* (CDF) of a random variable X, denoted as F(x), gives the probability that X will take a value less than or equal to x:  \n",
        "\\[ F(x) = P(X \\leq x) \\]  \n",
        "It is a non-decreasing, right-continuous function that ranges from 0 to 1 as x goes from negative infinity to positive infinity.  \n",
        "In contrast, the *PDF* (for continuous variables) describes the density at a specific point, but does not directly give probabilities. To find the probability that X falls within an interval, you integrate the PDF over that interval.  \n",
        "*In summary*, CDFs accumulate probabilities up to a point, while PDFs describe the density at a point.\n",
        "\n",
        "**6. What is a discrete uniform distribution?**  \n",
        "A *discrete uniform distribution* assigns equal probability to each of a finite set of outcomes. For example, rolling a fair six-sided die has a discrete uniform distribution over outcomes 1 through 6, with each outcome having probability \\( \\frac{1}{6} \\). Its key properties include:  \n",
        "- All outcomes are equally likely.  \n",
        "- The probability for each outcome is \\( \\frac{1}{n} \\), where n is the total number of outcomes.\n",
        "\n",
        "**7. What are the key properties of a Bernoulli distribution?**  \n",
        "The Bernoulli distribution models a single trial with two possible outcomes: success or failure. Its properties include:  \n",
        "- The random variable X takes values 1 (success) with probability p, and 0 (failure) with probability \\( 1 - p \\).  \n",
        "- The expected value (mean) is \\( E[X] = p \\).  \n",
        "- The variance is \\( Var(X) = p(1 - p) \\).  \n",
        "- It serves as the building block for other distributions like binomial.\n",
        "\n",
        "**8. What is the binomial distribution, and how is it used in probability?**  \n",
        "The binomial distribution models the number of successes in n independent Bernoulli trials, each with success probability p. Its probability mass function (PMF) is:  \n",
        "\\[ P(X = k) = \\binom{n}{k} p^{k} (1 - p)^{n - k} \\]  \n",
        "where \\( k = 0, 1, 2, ..., n \\).  \n",
        "It is used to calculate the probability of observing a certain number of successes, such as the probability of getting exactly 3 heads in 10 coin flips, or the probability of passing a certain number of students in a test.\n",
        "\n",
        "**9. What is the Poisson distribution and where is it applied?**  \n",
        "The Poisson distribution models the number of events occurring in a fixed interval or space when events happen independently, and the average rate is known. Its PMF is:  \n",
        "\\[ P(X = k) = \\frac{\\lambda^{k} e^{-\\lambda}}{k!} \\]  \n",
        "where \\( \\lambda \\) is the expected number of events.  \n",
        "Applications include modeling the number of phone calls received at a call center per hour, the number of decay events in radioactive material per unit time, or the number of emails received in an hour.\n",
        "\n",
        "**10. What is a continuous uniform distribution?**  \n",
        "A *continuous uniform distribution* assigns equal probability density over a specific interval \\([a, b]\\). The PDF is:  \n",
        "\\[ f(x) = \\frac{1}{b - a} \\quad \\text{for} \\quad a \\leq x \\leq b \\]  \n",
        "and zero elsewhere.  \n",
        "It models situations where all outcomes in the interval are equally likely, such as selecting a random point along a line segment.\n",
        "\n",
        "**11. What are the characteristics of a normal distribution?**  \n",
        "The normal distribution is symmetric and bell-shaped. Its key features include:  \n",
        "- Defined by mean \\( \\mu \\) (center) and standard deviation \\( \\sigma \\) (spread).  \n",
        "- The probability density function:  \n",
        "\\[ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{ -\\frac{(x - \\mu)^2}{2\\sigma^2} } \\]  \n",
        "- About 68% of data falls within 1 standard deviation of the mean, 95% within 2, and 99.7% within 3 (empirical rule).  \n",
        "- It models many natural phenomena due to the Central Limit Theorem.\n",
        "\n",
        "**12. What is the standard normal distribution, and why is it important?**  \n",
        "The *standard normal distribution* is a special case of the normal distribution with mean 0 and standard deviation 1. Its PDF simplifies to:  \n",
        "\\[ \\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{ -z^2/2 } \\]  \n",
        "It is crucial because it provides a reference for standardizing any normal variable through Z-scores, enabling comparison across different distributions and simplifying probability calculations.\n",
        "\n",
        "**13. What is the Central Limit Theorem (CLT), and why is it critical in statistics?**  \n",
        "The CLT states that, given a sufficiently large sample size, the sampling distribution of the sample mean will approximate a normal distribution, regardless of the population distribution's shape. This allows statisticians to make inferences about population parameters using normal probability models, even when the original data is not normally distributed.\n",
        "\n",
        "**14. How does the Central Limit Theorem relate to the normal distribution?**  \n",
        "The CLT explains why the distribution of the sample mean tends to be normal for large n. It justifies using normal distribution-based methods (like Z-tests) for hypothesis testing and confidence intervals, even when the underlying population is skewed or unknown.\n",
        "\n",
        "**15. What is the application of Z statistics in hypothesis testing?**  \n",
        "Z-statistics are used to test hypotheses about population parameters when the population variance is known or the sample size is large. They compare the observed sample statistic to the hypothesized population parameter, standardized by the standard error, to determine whether the difference is statistically significant.\n",
        "\n",
        "**16. How do you calculate a Z-score, and what does it represent?**  \n",
        "The Z-score is calculated as:  \n",
        "\\[ Z = \\frac{X - \\mu}{\\sigma} \\]  \n",
        "where:  \n",
        "- X is the observed value,  \n",
        "- \\( \\mu \\) is the population mean,  \n",
        "- \\( \\sigma \\) is the population standard deviation.  \n",
        "It represents how many standard deviations X is from the population mean, providing a standardized measure to compare values across different distributions.\n",
        "\n",
        "**17. What are point estimates and interval estimates in statistics?**  \n",
        "- *Point estimates* provide a single value estimate of a population parameter, such as the sample mean \\( \\bar{x} \\) estimating the population mean \\( \\mu \\).  \n",
        "- *Interval estimates* (confidence intervals) provide a range of plausible values for the parameter, such as \"the 95% confidence interval for \\( \\mu \\) is [lower, upper]\". They account for sampling variability and provide more information about the parameter's likely value.\n",
        "\n",
        "**18. What is the significance of confidence intervals in statistical analysis?**  \n",
        "Confidence intervals quantify the uncertainty associated with an estimate. They provide a range within which the true parameter is believed to lie with a specified confidence level (e.g., 95%). This helps in decision-making, showing the precision of estimates and whether parameters fall within acceptable bounds.\n",
        "\n",
        "**19. What is the relationship between a Z-score and a confidence interval?**  \n",
        "The Z-score corresponding to a confidence level determines the margin of error in the interval estimate. For example, for a 95% confidence level, the Z-score is approximately 1.96, which multiplies the standard error to find the interval's half-width. Larger Z-scores produce wider intervals, reflecting greater confidence.\n",
        "\n",
        "**20. How are Z-scores used to compare different distributions?**  \n",
        "By converting raw scores into Z-scores, values from different distributions are standardized to the standard normal scale. This allows direct comparison of how unusual or typical values are relative to their respective distributions, facilitating interpretation across different contexts.\n",
        "\n",
        "**21. What are the assumptions for applying the Central Limit Theorem?**  \n",
        "- The sample size should be sufficiently large (commonly n ≥ 30), or the underlying distribution should be approximately symmetric.  \n",
        "- The samples are independent of each other.  \n",
        "- The data are identically distributed with a finite mean and variance.  \n",
        "- The samples are randomly selected from the population.\n",
        "\n",
        "**22. What is the concept of expected value in a probability distribution?**  \n",
        "The *expected value* (or mean) of a random variable is the theoretical long-term average outcome if the experiment were repeated many times. It’s calculated as the sum (for discrete variables) or integral (for continuous variables) of possible outcomes weighted by their probabilities, providing a measure of the center of the distribution.\n",
        "\n",
        "**23. How does a probability distribution relate to the expected outcome of a random variable?**  \n",
        "The probability distribution describes all possible outcomes and their likelihoods. The *expected value* summarizes this distribution into a single number that indicates the average or most typical outcome, guiding predictions and decision-making based on the probabilistic model."
      ],
      "metadata": {
        "id": "zhwpIRNGZAN1"
      }
    }
  ]
}